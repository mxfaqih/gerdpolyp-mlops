{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1txh_t9BC0ga"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SKRIPSI - KLASIFIKASI CITRA ENDOSKOPI DENGAN CONVNEXT-TINY\n",
        "# TRAINING LOCAL COLAB (DATASET KAGGLE)\n",
        "# ============================================================\n",
        "\n",
        "# ============================================================\n",
        "# Step 1: Upload API Key Kaggle (kaggle.json)\n",
        "# ============================================================\n",
        "from google.colab import files\n",
        "print(\"Upload file 'kaggle.json'\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os, json\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"wb\") as f:\n",
        "    f.write(uploaded[\"kaggle.json\"])\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 600)\n",
        "print(\"‚úÖ Kaggle API key berhasil disimpan.\")\n",
        "\n",
        "# ============================================================\n",
        "# Step 2: Download Dataset dari Kaggle\n",
        "# ============================================================\n",
        "!pip install -q kaggle\n",
        "!kaggle datasets download -d huangthecreator/gastroendonet -p /content/\n",
        "!unzip -q /content/gastroendonet.zip -d /content/\n",
        "!mv /content/gastroendonet/dataset /content/dataset\n",
        "!rm -rf /content/gastroendonet*\n",
        "print(\"Dataset berhasil diunduh dan disiapkan di /content/dataset\")\n",
        "\n",
        "# Struktur:\n",
        "# /content/dataset/original/...\n",
        "# /content/dataset/augmented/...\n",
        "\n",
        "# ============================================================\n",
        "# üîπ Step 3: Mount Google Drive untuk backup hasil\n",
        "# ============================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ============================================================\n",
        "# üîπ Step 4: Konfigurasi Skenario\n",
        "# ============================================================\n",
        "import torch, random, numpy as np, os\n",
        "\n",
        "AUGMENT_ACTIVE   = True        # True -> gabung original + augmented | False -> hanya original\n",
        "NORMALIZE_ACTIVE = True        # True -> gunakan mean-std ImageNet\n",
        "BATCH_SIZE       = 32          # 16 / 32 / 64\n",
        "SEED             = 42          # seed global\n",
        "\n",
        "# Path lokal\n",
        "ROOT_DIR = \"/content\"\n",
        "DATA_ROOT = os.path.join(ROOT_DIR, \"dataset\")\n",
        "\n",
        "# Path hasil di local Colab\n",
        "scenario_id = f\"{'1a' if AUGMENT_ACTIVE else '1b'}-\" \\\n",
        "              f\"{'2a' if NORMALIZE_ACTIVE else '2b'}-\" \\\n",
        "              f\"{'3a' if BATCH_SIZE == 16 else '3b' if BATCH_SIZE == 32 else '3c'}\"\n",
        "\n",
        "RESULT_LOCAL = os.path.join(ROOT_DIR, \"results\", scenario_id)\n",
        "os.makedirs(RESULT_LOCAL, exist_ok=True)\n",
        "\n",
        "# Path hasil di Google Drive (untuk backup akhir)\n",
        "RESULT_DRIVE = f\"/content/drive/MyDrive/gerdpolyp-thesisv2/results/{scenario_id}\"\n",
        "os.makedirs(os.path.dirname(RESULT_DRIVE), exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "print(f\"üß™ Scenario: {scenario_id}\")\n",
        "print(f\"üìÇ Dataset: {DATA_ROOT}\")\n",
        "print(f\"üìÅ Output local: {RESULT_LOCAL}\")\n",
        "print(f\"‚öôÔ∏è Device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpUf8f0TDIeA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F30Bcj1eDKEP"
      },
      "outputs": [],
      "source": [
        "# --- PILIH DATASET ---\n",
        "if AUGMENT_ACTIVE:\n",
        "    data_dirs = [\n",
        "        os.path.join(DATA_ROOT, \"original\"),\n",
        "        os.path.join(DATA_ROOT, \"augmented\")\n",
        "    ]\n",
        "else:\n",
        "    data_dirs = [os.path.join(DATA_ROOT, \"original\")]\n",
        "\n",
        "# --- TRANSFORMASI ---\n",
        "if NORMALIZE_ACTIVE:\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "else:\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "# --- LOAD DATASET ---\n",
        "from torch.backends import cudnn\n",
        "cudnn.benchmark = True  # optimalkan GPU kernel\n",
        "datasets_list = [datasets.ImageFolder(d, transform=transform) for d in data_dirs]\n",
        "dataset = ConcatDataset(datasets_list)\n",
        "\n",
        "num_total = len(dataset)\n",
        "train_size = int(0.7 * num_total)\n",
        "val_size   = int(0.2 * num_total)\n",
        "test_size  = num_total - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset, [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(SEED)\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNFexpPHDQTn"
      },
      "outputs": [],
      "source": [
        "model = models.convnext_tiny(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "# Ganti classifier sesuai jumlah kelas (4)\n",
        "in_features = model.classifier[2].in_features\n",
        "model.classifier[2] = nn.Linear(in_features, 4)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Full fine-tuning\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-dbfg_dDROu"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20\n",
        "PATIENCE = 10\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "no_improve = 0\n",
        "history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0, 0, 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # --- VALIDASI ---\n",
        "    model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    history[\"train_loss\"].append(train_loss / len(train_loader))\n",
        "    history[\"val_loss\"].append(val_loss / len(val_loader))\n",
        "    history[\"train_acc\"].append(train_acc)\n",
        "    history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | \"\n",
        "          f\"TrainLoss: {train_loss/len(train_loader):.4f} | \"\n",
        "          f\"ValLoss: {val_loss/len(val_loader):.4f} | \"\n",
        "          f\"ValAcc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        no_improve = 0\n",
        "        torch.save(model.state_dict(), os.path.join(RESULT_LOCAL, \"model_best.pth\"))\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= PATIENCE:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "pd.DataFrame(history).to_csv(os.path.join(RESULT_LOCAL, \"history.csv\"), index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-O8mlRSDVz_"
      },
      "outputs": [],
      "source": [
        "# --- LOAD BEST MODEL ---\n",
        "model.load_state_dict(torch.load(os.path.join(RESULT_LOCAL, \"model_best.pth\")))\n",
        "model.eval()\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "prec = precision_score(y_true, y_pred, average=\"macro\")\n",
        "rec  = recall_score(y_true, y_pred, average=\"macro\")\n",
        "f1   = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "metrics = {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
        "with open(os.path.join(RESULT_LOCAL, \"metrics_test.json\"), \"w\") as f:\n",
        "    json.dump(metrics, f, indent=4)\n",
        "\n",
        "print(\"\\n Test Results:\")\n",
        "for k, v in metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "# --- CONFUSION MATRIX ---\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='magma')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULT_LOCAL, \"confusion_matrix.png\"))\n",
        "plt.show()\n",
        "\n",
        "# --- LOSS CURVE ---\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULT_LOCAL, \"loss_curve.png\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v2Pne_QDhPO"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "print(\"Menyalin hasil ke Google Drive...\")\n",
        "\n",
        "if os.path.exists(RESULT_DRIVE):\n",
        "    shutil.rmtree(RESULT_DRIVE)\n",
        "shutil.copytree(RESULT_LOCAL, RESULT_DRIVE)\n",
        "\n",
        "print(f\"Hasil skenario {scenario_id} berhasil disalin ke Drive:\")\n",
        "print(RESULT_DRIVE)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
