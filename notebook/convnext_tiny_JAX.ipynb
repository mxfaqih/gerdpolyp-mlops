{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "uDIhPwL6iZ1F"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, lax\n",
        "import functools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6PZ5x93jCBF"
      },
      "source": [
        "Helper: GELU, LayerNorm, conv2d & depthwise conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "URllhbk4i-3D"
      },
      "outputs": [],
      "source": [
        "def gelu(x):\n",
        "    return 0.5 * x * (1.0 + jnp.tanh(jnp.sqrt(2.0 / jnp.pi) * (x + 0.044715 * x**3)))\n",
        "\n",
        "\n",
        "def layer_norm(x, gamma, beta, eps=1e-5):\n",
        "    \"\"\"\n",
        "    x: (..., C)  (channels-last)\n",
        "    gamma, beta: (C,)\n",
        "    \"\"\"\n",
        "    mean = jnp.mean(x, axis=-1, keepdims=True)\n",
        "    var = jnp.var(x, axis=-1, keepdims=True)\n",
        "    x_hat = (x - mean) / jnp.sqrt(var + eps)\n",
        "    return gamma * x_hat + beta\n",
        "\n",
        "\n",
        "def conv2d(x, w, stride=(1, 1), padding=\"SAME\", feature_group_count=1):\n",
        "    \"\"\"\n",
        "    x: (N, H, W, C_in)\n",
        "    w: (KH, KW, C_in, C_out)\n",
        "    \"\"\"\n",
        "    return lax.conv_general_dilated(\n",
        "        lhs=x,\n",
        "        rhs=w,\n",
        "        window_strides=stride,\n",
        "        padding=padding,\n",
        "        dimension_numbers=(\"NHWC\", \"HWIO\", \"NHWC\"),\n",
        "        feature_group_count=feature_group_count,\n",
        "    )\n",
        "\n",
        "\n",
        "def depthwise_conv2d(x, w, stride=(1, 1), padding=\"SAME\"):\n",
        "    \"\"\"\n",
        "    Depthwise conv:\n",
        "    x: (N, H, W, C)\n",
        "    w: (KH, KW, 1, C)\n",
        "    feature_group_count = C\n",
        "    \"\"\"\n",
        "    C = x.shape[-1]\n",
        "    return conv2d(x, w, stride=stride, padding=padding, feature_group_count=C)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Penjelasan:**\n",
        "\n",
        "* `def gelu(x):`\n",
        "  Mendefinisikan fungsi `gelu` yang menerima tensor `x` (bentuk arbitrer, biasanya `(N, ..., C)`).\n",
        "\n",
        "* `return 0.5 * x * (1.0 + jnp.tanh(...))`\n",
        "  Ini implementasi aproksimasi GELU yang populer (Hendrycks & Gimpel).\n",
        "  Di dalamnya:\n",
        "\n",
        "  * `x**3` → pangkat tiga tiap elemen `x`.\n",
        "  * `0.044715 * x**3` → konstanta empiris untuk akurasi aproksimasi.\n",
        "  * `(x + 0.044715 * x**3)` → argumen untuk fungsi `tanh`.\n",
        "  * `jnp.sqrt(2.0 / jnp.pi)` → konstanta (\\sqrt{2/\\pi}).\n",
        "  * `jnp.tanh(jnp.sqrt(2.0 / jnp.pi) * (...))` → tanh dari argumen tadi.\n",
        "  * `1.0 + tanh(...)` → shift supaya dalam range kira-kira [0, 2].\n",
        "  * `0.5 * x * (1 + tanh(...))` → skema akhir GELU aproksimasi.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**LayerNorm:**\n",
        "\n",
        "* `def layer_norm(x, gamma, beta, eps=1e-5):`\n",
        "  Mendefinisikan LayerNorm generik:\n",
        "\n",
        "  * `x`: tensor dengan channel di dimensi terakhir, misalnya `(N, H, W, C)` atau `(N, C)`.\n",
        "  * `gamma`, `beta`: parameter skala dan bias per channel, shape `(C,)`.\n",
        "  * `eps`: konstanta kecil untuk menghindari pembagian nol.\n",
        "\n",
        "* `\"\"\" x: (..., C) ... \"\"\"`\n",
        "  Docstring: menjelaskan bahwa `x` punya dimensi terakhir = channels.\n",
        "\n",
        "* `mean = jnp.mean(x, axis=-1, keepdims=True)`\n",
        "\n",
        "  * Menghitung rata-rata (`mean`) sepanjang dimensi channel (dimensi terakhir).\n",
        "  * `axis=-1` → pakai dimensi terakhir.\n",
        "  * `keepdims=True` → supaya hasilnya bisa di-*broadcast* kembali ke `x` (shape jadi `(..., 1)`).\n",
        "\n",
        "* `var = jnp.var(x, axis=-1, keepdims=True)`\n",
        "\n",
        "  * Menghitung varians per posisi (untuk LN) sepanjang channel.\n",
        "\n",
        "* `x_hat = (x - mean) / jnp.sqrt(var + eps)`\n",
        "\n",
        "  * Normalisasi: kurangi mean dan bagi akar varians + eps.\n",
        "  * `x_hat` sekarang punya mean 0 dan var 1 di setiap posisi (per sample/pixel).\n",
        "\n",
        "* `return gamma * x_hat + beta`\n",
        "\n",
        "  * `gamma` dan `beta` shape `(C,)`, akan otomatis *broadcast* ke `(..., C)`.\n",
        "  * Menghasilkan output LN yang sudah distandarisasi dan di-skala/bias.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conv2D:**\n",
        "\n",
        "* `def conv2d(x, w, stride=(1, 1), padding=\"SAME\", feature_group_count=1):`\n",
        "\n",
        "  * Wrapper sederhana untuk konvolusi 2D JAX.\n",
        "  * `x`: input tensor bentuk `(N, H, W, C_in)` (NHWC).\n",
        "  * `w`: weight kernel `(KH, KW, C_in, C_out)`.\n",
        "  * `stride`: tuple `(stride_h, stride_w)`.\n",
        "  * `padding`: `\"SAME\"` atau `\"VALID\"`.\n",
        "  * `feature_group_count`: untuk group conv (dipakai di depthwise).\n",
        "\n",
        "* Docstring menjelaskan shape `x` dan `w`.\n",
        "\n",
        "* `return lax.conv_general_dilated(...`\n",
        "\n",
        "  * `lhs=x`: input (left-hand side).\n",
        "  * `rhs=w`: filter/kernel.\n",
        "  * `window_strides=stride`: stride.\n",
        "  * `padding=padding`: scheme padding (string).\n",
        "  * `dimension_numbers=(\"NHWC\", \"HWIO\", \"NHWC\")`:\n",
        "\n",
        "    * Format input: `\"NHWC\"`\n",
        "    * Format kernel: `\"HWIO\"` (Height, Width, InputChan, OutputChan)\n",
        "    * Format output: `\"NHWC\"`.\n",
        "  * `feature_group_count=feature_group_count`:\n",
        "\n",
        "    * Jika 1 → conv standard.\n",
        "    * Jika `C_in` → depthwise (tiap channel di-*group* sendiri).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Penjelasan:**\n",
        "\n",
        "* `def depthwise_conv2d(x, w, stride=(1, 1), padding=\"SAME\"):`\n",
        "  Fungsi khusus untuk depthwise convolution:\n",
        "\n",
        "* Docstring:\n",
        "\n",
        "  * `x` shape `(N, H, W, C)`.\n",
        "  * `w` shape `(KH, KW, C, 1)` → satu filter per channel input, menghasilkan 1 channel output per channel → total tetap `C`.\n",
        "\n",
        "* `C = x.shape[-1]`\n",
        "  Mengambil jumlah channel dari dimensi terakhir tensor `x`.\n",
        "\n",
        "* `return conv2d(... feature_group_count=C)`\n",
        "\n",
        "  * Menggunakan `conv2d` umum, tapi `feature_group_count=C`.\n",
        "  * Ini berarti setiap channel diproses oleh grup terpisah → **depthwise convolution**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMH9NDTQjNk8"
      },
      "source": [
        "Init weight Simplified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "H87k2ofrjDxU"
      },
      "outputs": [],
      "source": [
        "def init_conv_weight(key, ksize, in_ch, out_ch):\n",
        "    kh, kw = ksize\n",
        "    fan_in = kh * kw * in_ch\n",
        "    std = 1.0 / jnp.sqrt(fan_in)\n",
        "    w = std * random.normal(key, (kh, kw, in_ch, out_ch))\n",
        "    return w\n",
        "\n",
        "\n",
        "def init_depthwise_weight(key, ksize, channels):\n",
        "    kh, kw = ksize\n",
        "    fan_in = kh * kw  # per-channel, in_channels_per_group = 1\n",
        "    std = 1.0 / jnp.sqrt(fan_in)\n",
        "    # (KH, KW, in_channels_per_group=1, out_channels_per_group=1) for each of C groups\n",
        "    # Tapi karena feature_group_count = C, total out_channels = C * 1 = C\n",
        "    w = std * random.normal(key, (kh, kw, 1, channels))\n",
        "    return w\n",
        "\n",
        "\n",
        "\n",
        "def init_dense_weight(key, in_dim, out_dim):\n",
        "    fan_in = in_dim\n",
        "    std = 1.0 / jnp.sqrt(fan_in)\n",
        "    w = std * random.normal(key, (in_dim, out_dim))\n",
        "    b = jnp.zeros((out_dim,))\n",
        "    return w, b\n",
        "\n",
        "\n",
        "def init_layer_norm_params(dim):\n",
        "    gamma = jnp.ones((dim,))\n",
        "    beta = jnp.zeros((dim,))\n",
        "    return {\"gamma\": gamma, \"beta\": beta}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Penjelasan:**\n",
        "\n",
        "* `def init_conv_weight(key, ksize, in_ch, out_ch):`\n",
        "\n",
        "  * Fungsi untuk inisialisasi kernel conv biasa.\n",
        "  * `key`: PRNGKey JAX untuk random.\n",
        "  * `ksize`: tuple `(kh, kw)`.\n",
        "  * `in_ch`, `out_ch`: jumlah channel.\n",
        "\n",
        "* `kh, kw = ksize`\n",
        "  Pecah ukuran kernel menjadi tinggi dan lebar.\n",
        "\n",
        "* `fan_in = kh * kw * in_ch`\n",
        "\n",
        "  * Fan-in = banyaknya input per neuron (per output channel per posisi).\n",
        "\n",
        "* `std = 1.0 / jnp.sqrt(fan_in)`\n",
        "\n",
        "  * Standar deviasi untuk inisialisasi (mirip Xavier/He sederhana).\n",
        "\n",
        "* `w = std * random.normal(key, (kh, kw, in_ch, out_ch))`\n",
        "\n",
        "  * Generate tensor normal N(0,1) lalu skala dengan `std`.\n",
        "  * Shape sesuai kernel conv: `(KH, KW, C_in, C_out)`.\n",
        "\n",
        "* `return w`\n",
        "\n",
        "  * Mengembalikan weight conv.\n",
        "\n",
        "* Sama seperti `init_conv_weight`, tapi khusus depthwise:\n",
        "\n",
        "  * `channels`: jumlah channel input.\n",
        "  * `fan_in = kh * kw * 1` karena per channel bekerja sendiri.\n",
        "  * Shape output: `(KH, KW, C, 1)` sesuai depthwise conv.\n",
        "\n",
        "---\n",
        "\n",
        "**Penjelasan:**\n",
        "\n",
        "* `def init_dense_weight(key, in_dim, out_dim):`\n",
        "\n",
        "  * Inisialisasi matrix weight dan bias untuk layer dense/linear.\n",
        "\n",
        "* `fan_in = in_dim` → jumlah input per neuron.\n",
        "\n",
        "* `std = 1.0 / jnp.sqrt(fan_in)` → scaling.\n",
        "\n",
        "* `w = std * random.normal(key, (in_dim, out_dim))`\n",
        "\n",
        "  * Weight matrix, shape `(in_dim, out_dim)`.\n",
        "\n",
        "* `b = jnp.zeros((out_dim,))`\n",
        "\n",
        "  * Bias vector, awalnya semua nol.\n",
        "\n",
        "* `return w, b`\n",
        "\n",
        "  * Mengembalikan pasangan weight & bias.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b1Qvo3pjiN1"
      },
      "source": [
        "ConvNeXt Block (DW 7×7, LN, MLP 4×, LayerScale, residual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "BWaZiR6pjgix"
      },
      "outputs": [],
      "source": [
        "def init_convnext_block_params(key, dim, mlp_ratio=4, layer_scale_init=1e-6):\n",
        "    \"\"\"\n",
        "    ConvNeXt block:\n",
        "    - DWConv 7x7\n",
        "    - LayerNorm (channels-last)\n",
        "    - MLP: dim -> 4*dim -> dim\n",
        "    - LayerScale (gamma)\n",
        "    - Residual\n",
        "    \"\"\"\n",
        "    k1, k2, k3 = random.split(key, 3)\n",
        "    # depthwise conv\n",
        "    dw_w = init_depthwise_weight(k1, (7, 7), dim)\n",
        "\n",
        "    # layernorm params\n",
        "    ln = init_layer_norm_params(dim)\n",
        "\n",
        "    # MLP\n",
        "    hidden_dim = dim * mlp_ratio\n",
        "    w1, b1 = init_dense_weight(k2, dim, hidden_dim)\n",
        "    w2, b2 = init_dense_weight(k3, hidden_dim, dim)\n",
        "\n",
        "    # layer scale (per-channel)\n",
        "    layer_scale = jnp.ones((dim,)) * layer_scale_init\n",
        "\n",
        "    return {\n",
        "        \"dw_conv\": {\"w\": dw_w},\n",
        "        \"ln\": ln,\n",
        "        \"mlp\": {\n",
        "            \"w1\": w1,\n",
        "            \"b1\": b1,\n",
        "            \"w2\": w2,\n",
        "            \"b2\": b2,\n",
        "        },\n",
        "        \"layer_scale\": layer_scale,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9EuqSEHj4l7"
      },
      "source": [
        "Forward block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "FNKFqjMGj7Vp"
      },
      "outputs": [],
      "source": [
        "def convnext_block_forward(params, x):\n",
        "    \"\"\"\n",
        "    x: (N, H, W, C)\n",
        "    \"\"\"\n",
        "    shortcut = x\n",
        "    dim = x.shape[-1]\n",
        "\n",
        "    # 1) Depthwise conv 7x7\n",
        "    w_dw = params[\"dw_conv\"][\"w\"]\n",
        "    x = depthwise_conv2d(x, w_dw, stride=(1, 1), padding=\"SAME\")\n",
        "\n",
        "    # 2) LayerNorm (channels-last)\n",
        "    ln_params = params[\"ln\"]\n",
        "    x = layer_norm(x, ln_params[\"gamma\"], ln_params[\"beta\"])\n",
        "\n",
        "    # 3) MLP: (N,H,W,C) -> (N*H*W, C)\n",
        "    N, H, W, C = x.shape\n",
        "    x_flat = x.reshape(N * H * W, C)\n",
        "\n",
        "    mlp = params[\"mlp\"]\n",
        "    x_flat = jnp.dot(x_flat, mlp[\"w1\"]) + mlp[\"b1\"]\n",
        "    x_flat = gelu(x_flat)\n",
        "    x_flat = jnp.dot(x_flat, mlp[\"w2\"]) + mlp[\"b2\"]\n",
        "\n",
        "    x = x_flat.reshape(N, H, W, C)\n",
        "\n",
        "    # 4) LayerScale + residual\n",
        "    gamma = params[\"layer_scale\"]  # (C,)\n",
        "    x = shortcut + gamma * x\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Forward:**\n",
        "\n",
        "* `def convnext_block_forward(params, x):`\n",
        "\n",
        "  * Fungsi forward untuk satu block.\n",
        "  * `params`: dict dari `init_convnext_block_params`.\n",
        "  * `x`: feature map `(N, H, W, C)`.\n",
        "\n",
        "* Docstring menjelaskan shape `x`.\n",
        "\n",
        "* `shortcut = x`\n",
        "\n",
        "  * Menyimpan input original untuk jalur residual.\n",
        "\n",
        "* `dim = x.shape[-1]`\n",
        "\n",
        "  * Mengambil jumlah channel C (meski tidak dipakai eksplisit, cuma referensi).\n",
        "\n",
        "---\n",
        "\n",
        "**1) Depthwise Conv**\n",
        "\n",
        "* `w_dw = params[\"dw_conv\"][\"w\"]`\n",
        "\n",
        "  * Ambil kernel depthwise dari dict params.\n",
        "\n",
        "* `x = depthwise_conv2d(x, w_dw, stride=(1, 1), padding=\"SAME\")`\n",
        "\n",
        "  * Terapkan depthwise conv 7×7 dengan stride 1, padding “SAME”.\n",
        "  * Setelah ini, shape `x` tetap `(N, H, W, C)`.\n",
        "\n",
        "---\n",
        "\n",
        "**2) LayerNorm**\n",
        "\n",
        "* `ln_params = params[\"ln\"]`\n",
        "\n",
        "  * Ambil param LN `{gamma, beta}`.\n",
        "\n",
        "* `x = layer_norm(x, ln_params[\"gamma\"], ln_params[\"beta\"])`\n",
        "\n",
        "  * Terapkan LayerNorm di dimensi channel.\n",
        "\n",
        "---\n",
        "\n",
        "**3) MLP (per posisi pixel)**\n",
        "\n",
        "* `N, H, W, C = x.shape`\n",
        "\n",
        "  * Unpack dim, supaya bisa reshape.\n",
        "\n",
        "* `x_flat = x.reshape(N * H * W, C)`\n",
        "\n",
        "  * Flatten semua posisi `(H,W)` dan batch `N` ke satu dimensi besar.\n",
        "  * Kita akan menganggap setiap posisi `(n,h,w)` sebagai satu vektor `C` dan melewati MLP sama untuk semua posisi.\n",
        "\n",
        "* `mlp = params[\"mlp\"]`\n",
        "\n",
        "  * Shortcut ke dict MLP `{w1, b1, w2, b2}`.\n",
        "\n",
        "* `x_flat = jnp.dot(x_flat, mlp[\"w1\"]) + mlp[\"b1\"]`\n",
        "\n",
        "  * Linear pertama: `(N*H*W, C) · (C, 4C) → (N*H*W, 4C)`.\n",
        "\n",
        "* `x_flat = gelu(x_flat)`\n",
        "\n",
        "  * Aktivasi GELU di hidden.\n",
        "\n",
        "* `x_flat = jnp.dot(x_flat, mlp[\"w2\"]) + mlp[\"b2\"]`\n",
        "\n",
        "  * Linear kedua: `(N*H*W, 4C) · (4C, C) → (N*H*W, C)`.\n",
        "\n",
        "* `x = x_flat.reshape(N, H, W, C)`\n",
        "\n",
        "  * Kembalikan ke bentuk spasial semula.\n",
        "\n",
        "---\n",
        "\n",
        "**4) LayerScale + Residual**\n",
        "\n",
        "* `gamma = params[\"layer_scale\"]  # (C,)`\n",
        "\n",
        "  * Ambil vektor gamma per channel.\n",
        "\n",
        "* `x = shortcut + gamma * x`\n",
        "\n",
        "  * `gamma * x` → meng-scale output block per channel.\n",
        "  * Tambah `shortcut` → residual connection.\n",
        "\n",
        "* `return x`\n",
        "\n",
        "  * Output block: `(N, H, W, C)` dengan fitur yang sudah diproses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPKvucmnj-3P"
      },
      "source": [
        "Downsample layer & stem & head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "QdK69lauj-sj"
      },
      "outputs": [],
      "source": [
        "def init_downsample_layer_params(key, in_dim, out_dim):\n",
        "    k1, k2 = random.split(key)\n",
        "    ln = init_layer_norm_params(in_dim)\n",
        "    w = init_conv_weight(k2, (2, 2), in_dim, out_dim)\n",
        "    b = jnp.zeros((out_dim,))\n",
        "    return {\n",
        "        \"ln\": ln,\n",
        "        \"conv\": {\"w\": w, \"b\": b},\n",
        "    }\n",
        "\n",
        "\n",
        "def downsample_forward(params, x):\n",
        "    \"\"\"\n",
        "    x: (N, H, W, C_in)\n",
        "    \"\"\"\n",
        "    ln = params[\"ln\"]\n",
        "    x = layer_norm(x, ln[\"gamma\"], ln[\"beta\"])\n",
        "    w = params[\"conv\"][\"w\"]\n",
        "    b = params[\"conv\"][\"b\"]\n",
        "    x = conv2d(x, w, stride=(2, 2), padding=\"SAME\")\n",
        "    x = x + b  # broadcast\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Downsample:**\n",
        "\n",
        "* `def init_downsample_layer_params(key, in_dim, out_dim):`\n",
        "\n",
        "  * Inisialisasi layer downsample antara dua stage.\n",
        "  * `in_dim`: C_in.\n",
        "  * `out_dim`: C_out.\n",
        "\n",
        "* `k1, k2 = random.split(key)`\n",
        "\n",
        "  * `k1` → LN (meski LN tidak random).\n",
        "  * `k2` → conv 2×2.\n",
        "\n",
        "* `ln = init_layer_norm_params(in_dim)`\n",
        "\n",
        "  * LN untuk channel `in_dim`.\n",
        "\n",
        "* `w = init_conv_weight(k2, (2, 2), in_dim, out_dim)`\n",
        "\n",
        "  * Kernel conv 2×2 yang mengubah channel `in_dim → out_dim`.\n",
        "\n",
        "* `b = jnp.zeros((out_dim,))`\n",
        "\n",
        "  * Bias conv layer.\n",
        "\n",
        "* `return { \"ln\": ln, \"conv\": {\"w\": w, \"b\": b} }`\n",
        "\n",
        "  * Struktur params downsample: LN + conv.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "XNijjmkgkDod"
      },
      "outputs": [],
      "source": [
        "def init_stem_params(key, in_ch=3, dim=96):\n",
        "    w = init_conv_weight(key, (4, 4), in_ch, dim)\n",
        "    b = jnp.zeros((dim,))\n",
        "    return {\"w\": w, \"b\": b}\n",
        "\n",
        "\n",
        "def stem_forward(params, x):\n",
        "    \"\"\"\n",
        "    x: (N, H, W, 3), pixel [0,1] or [0,255] (silakan normalisasi sendiri)\n",
        "    \"\"\"\n",
        "    w = params[\"w\"]\n",
        "    b = params[\"b\"]\n",
        "    x = conv2d(x, w, stride=(4, 4), padding=\"SAME\")\n",
        "    x = x + b\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eIV1Iv-kFqE"
      },
      "source": [
        "global avg pool + LN + FC ke num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "XTh1n4HCkEB_"
      },
      "outputs": [],
      "source": [
        "def init_head_params(key, dim, num_classes):\n",
        "    k1, k2 = random.split(key)\n",
        "    ln = init_layer_norm_params(dim)\n",
        "    w, b = init_dense_weight(k2, dim, num_classes)\n",
        "    return {\"ln\": ln, \"fc\": {\"w\": w, \"b\": b}}\n",
        "\n",
        "\n",
        "def head_forward(params, x):\n",
        "    \"\"\"\n",
        "    x: (N, H, W, C)\n",
        "    \"\"\"\n",
        "    # Global average pooling\n",
        "    x = jnp.mean(x, axis=(1, 2))  # (N, C)\n",
        "\n",
        "    # LN over channels\n",
        "    ln = params[\"ln\"]\n",
        "    x = layer_norm(x, ln[\"gamma\"], ln[\"beta\"])\n",
        "\n",
        "    # Linear classifier\n",
        "    w = params[\"fc\"][\"w\"]\n",
        "    b = params[\"fc\"][\"b\"]\n",
        "    logits = jnp.dot(x, w) + b\n",
        "    return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njs7yLcwkLpR"
      },
      "source": [
        "Full ConvNeXt-Tiny params & forward\n",
        "Konfigurasi ConvNeXt-Tiny\n",
        "\n",
        "Depths: [3, 3, 9, 3]\n",
        "\n",
        "Dims: [96, 192, 384, 768]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "mSDRELdKcFFk"
      },
      "outputs": [],
      "source": [
        "CONVNEXT_TINY_DEPTHS = [3, 3, 9, 3]\n",
        "CONVNEXT_TINY_DIMS   = [96, 192, 384, 768]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "QOOFGJhVkJL3"
      },
      "outputs": [],
      "source": [
        "def init_convnext_tiny_params(key, num_classes=1000, in_ch=3):\n",
        "    \"\"\"\n",
        "    Return: nested dict: {'stem', 'stages', 'head'}\n",
        "    \"\"\"\n",
        "    depths = CONVNEXT_TINY_DEPTHS\n",
        "    dims = CONVNEXT_TINY_DIMS\n",
        "\n",
        "    keys = random.split(key, 2 + sum(depths) + (len(dims) - 1) + 1)\n",
        "    k_idx = 0\n",
        "\n",
        "    # Stem\n",
        "    stem_key = keys[k_idx]; k_idx += 1\n",
        "    stem = init_stem_params(stem_key, in_ch=in_ch, dim=dims[0])\n",
        "\n",
        "    stages = []\n",
        "    block_keys_needed = sum(depths)\n",
        "    block_keys = keys[k_idx: k_idx + block_keys_needed]\n",
        "    k_idx += block_keys_needed\n",
        "\n",
        "    # Downsample layers (between stages)\n",
        "    downsample_keys = keys[k_idx: k_idx + (len(dims) - 1)]\n",
        "    k_idx += (len(dims) - 1)\n",
        "\n",
        "    # Build stages\n",
        "    bk = 0\n",
        "    for stage_idx, (depth, dim) in enumerate(zip(depths, dims)):\n",
        "        # Blocks in this stage\n",
        "        blocks = []\n",
        "        for _ in range(depth):\n",
        "            blocks.append(\n",
        "                init_convnext_block_params(block_keys[bk], dim=dim, mlp_ratio=4, layer_scale_init=1e-6)\n",
        "            )\n",
        "            bk += 1\n",
        "\n",
        "        # Downsample from previous stage to this stage (except first)\n",
        "        if stage_idx == 0:\n",
        "            downsample = None\n",
        "        else:\n",
        "            in_dim = dims[stage_idx - 1]\n",
        "            out_dim = dim\n",
        "            ds_params = init_downsample_layer_params(downsample_keys[stage_idx - 1], in_dim, out_dim)\n",
        "            downsample = ds_params\n",
        "\n",
        "        stages.append(\n",
        "            {\n",
        "                \"blocks\": blocks,\n",
        "                \"downsample\": downsample,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # Head\n",
        "    head_key = keys[k_idx]; k_idx += 1\n",
        "    head = init_head_params(head_key, dim=dims[-1], num_classes=num_classes)\n",
        "\n",
        "    return {\n",
        "        \"stem\": stem,\n",
        "        \"stages\": stages,\n",
        "        \"head\": head,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NpfibNrlVpO"
      },
      "source": [
        "Forward ConvNeXt-Tiny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "KIReml3NlUBa"
      },
      "outputs": [],
      "source": [
        "def convnext_tiny_forward(params, x, train=False):\n",
        "    \"\"\"\n",
        "    x: (N, H, W, 3)\n",
        "    return: logits (N, num_classes)\n",
        "    train flag currently unused (no dropout/stochastic depth),\n",
        "           tapi kamu bisa extend nanti.\n",
        "    \"\"\"\n",
        "    # Stem\n",
        "    x = stem_forward(params[\"stem\"], x)\n",
        "\n",
        "    # Stages\n",
        "    stages = params[\"stages\"]\n",
        "    for i, stage in enumerate(stages):\n",
        "        if stage[\"downsample\"] is not None:\n",
        "            x = downsample_forward(stage[\"downsample\"], x)\n",
        "\n",
        "        for block_params in stage[\"blocks\"]:\n",
        "            x = convnext_block_forward(block_params, x)\n",
        "\n",
        "    # Head\n",
        "    logits = head_forward(params[\"head\"], x)\n",
        "    return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4lhcsW2lazR"
      },
      "source": [
        "Training dengan autograd (AdamW manual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "at0W3HuylahN"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(logits, labels):\n",
        "    \"\"\"\n",
        "    logits: (N, num_classes)\n",
        "    labels: (N,) int32\n",
        "    \"\"\"\n",
        "    log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
        "    nll = -log_probs[jnp.arange(labels.shape[0]), labels]\n",
        "    return jnp.mean(nll)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "zHoPCZHildox"
      },
      "outputs": [],
      "source": [
        "def loss_fn(params, x, y):\n",
        "    logits = convnext_tiny_forward(params, x, train=True)\n",
        "    return cross_entropy_loss(logits, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFiiMjoPlj9x"
      },
      "source": [
        "AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "UC4omavAlksb"
      },
      "outputs": [],
      "source": [
        "def init_adamw_state(params, lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.05):\n",
        "    m = jax.tree.map(jnp.zeros_like, params)\n",
        "    v = jax.tree.map(jnp.zeros_like, params)\n",
        "    return {\n",
        "        \"m\": m,\n",
        "        \"v\": v,\n",
        "        \"t\": jnp.array(0, dtype=jnp.int32),\n",
        "        \"lr\": lr,\n",
        "        \"beta1\": beta1,\n",
        "        \"beta2\": beta2,\n",
        "        \"eps\": eps,\n",
        "        \"weight_decay\": weight_decay,\n",
        "    }\n",
        "\n",
        "\n",
        "def adamw_update(params, grads, opt_state):\n",
        "    beta1 = opt_state[\"beta1\"]\n",
        "    beta2 = opt_state[\"beta2\"]\n",
        "    eps = opt_state[\"eps\"]\n",
        "    lr = opt_state[\"lr\"]\n",
        "    wd = opt_state[\"weight_decay\"]\n",
        "\n",
        "    t = opt_state[\"t\"] + 1\n",
        "\n",
        "    m = jax.tree.map(lambda m, g: beta1 * m + (1 - beta1) * g, opt_state[\"m\"], grads)\n",
        "    v = jax.tree.map(lambda v, g: beta2 * v + (1 - beta2) * (g * g), opt_state[\"v\"], grads)\n",
        "\n",
        "    m_hat = jax.tree.map(lambda m: m / (1 - beta1**t), m)\n",
        "    v_hat = jax.tree.map(lambda v: v / (1 - beta2**t), v)\n",
        "\n",
        "    def update_param(p, mh, vh):\n",
        "        # Adam step + decoupled weight decay\n",
        "        adam_step = mh / (jnp.sqrt(vh) + eps)\n",
        "        return p - lr * (adam_step + wd * p)\n",
        "\n",
        "    new_params = jax.tree.map(update_param, params, m_hat, v_hat)\n",
        "\n",
        "    new_opt_state = {\n",
        "        \"m\": m,\n",
        "        \"v\": v,\n",
        "        \"t\": t,\n",
        "        \"lr\": lr,\n",
        "        \"beta1\": beta1,\n",
        "        \"beta2\": beta2,\n",
        "        \"eps\": eps,\n",
        "        \"weight_decay\": wd,\n",
        "    }\n",
        "    return new_params, new_opt_state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrXwut2DmEsj"
      },
      "source": [
        "Train step (jit + autograd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Gg9AJ9VFmCYN"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(params, opt_state, x, y):\n",
        "    \"\"\"\n",
        "    x: (N, H, W, 3) float32\n",
        "    y: (N,) int32 labels\n",
        "    \"\"\"\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
        "    new_params, new_opt_state = adamw_update(params, grads, opt_state)\n",
        "    return new_params, new_opt_state, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuWOupLJmISU"
      },
      "source": [
        "Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "JHeaNx_zmGyM"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def predict(params, x):\n",
        "    \"\"\"\n",
        "    x: (N, H, W, 3)\n",
        "    return: predicted class (N,)\n",
        "    \"\"\"\n",
        "    logits = convnext_tiny_forward(params, x, train=False)\n",
        "    return jnp.argmax(logits, axis=-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPAe7iztmLAO"
      },
      "source": [
        "Ex:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C90VyktQmKzL",
        "outputId": "3905e918-9774-4c44-f745-d859573a9432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 2.5652852058410645\n",
            "preds: [0 1 2 3]\n"
          ]
        }
      ],
      "source": [
        "key = random.PRNGKey(42)\n",
        "\n",
        "# 1) Init params ConvNeXt-Tiny untuk, misalnya, CIFAR-10 (10 kelas)\n",
        "num_classes = 10\n",
        "params = init_convnext_tiny_params(key, num_classes=num_classes, in_ch=3)\n",
        "\n",
        "# 2) Init optimizer state\n",
        "opt_state = init_adamw_state(params, lr=1e-3, weight_decay=0.05)\n",
        "\n",
        "# 3) Dummy batch (misal sudah di-resize ke 224x224 dan dinormalisasi)\n",
        "batch_size = 4\n",
        "x_dummy = random.normal(key, (batch_size, 224, 224, 3))  # contoh\n",
        "y_dummy = jnp.array([0, 1, 2, 3], dtype=jnp.int32)\n",
        "\n",
        "# 4) Satu step training\n",
        "params, opt_state, loss_val = train_step(params, opt_state, x_dummy, y_dummy)\n",
        "print(\"loss:\", float(loss_val))\n",
        "\n",
        "# 5) Inference\n",
        "preds = predict(params, x_dummy)\n",
        "print(\"preds:\", preds)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
